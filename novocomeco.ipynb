{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T03:40:16.983401Z","iopub.execute_input":"2024-07-25T03:40:16.984034Z","iopub.status.idle":"2024-07-25T03:40:17.349046Z","shell.execute_reply.started":"2024-07-25T03:40:16.984004Z","shell.execute_reply":"2024-07-25T03:40:17.348184Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:40:25.383866Z","iopub.execute_input":"2024-07-25T03:40:25.384746Z","iopub.status.idle":"2024-07-25T03:42:22.908125Z","shell.execute_reply.started":"2024-07-25T03:40:25.384709Z","shell.execute_reply":"2024-07-25T03:42:22.906815Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\n\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:42:31.946270Z","iopub.execute_input":"2024-07-25T03:42:31.947282Z","iopub.status.idle":"2024-07-25T03:42:49.514618Z","shell.execute_reply.started":"2024-07-25T03:42:31.947239Z","shell.execute_reply":"2024-07-25T03:42:49.513631Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-25 03:42:38.446051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-25 03:42:38.446174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-25 03:42:38.574294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"hugfac\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:46:38.729696Z","iopub.execute_input":"2024-07-25T03:46:38.730341Z","iopub.status.idle":"2024-07-25T03:46:58.116563Z","shell.execute_reply.started":"2024-07-25T03:46:38.730304Z","shell.execute_reply":"2024-07-25T03:46:58.115199Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msecaio\u001b[0m (\u001b[33msecaio-software\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240725_034641-5djlbs1k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5djlbs1k' target=\"_blank\">gallant-night-7</a></strong> to <a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5djlbs1k' target=\"_blank\">https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5djlbs1k</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:47:07.103624Z","iopub.execute_input":"2024-07-25T03:47:07.103974Z","iopub.status.idle":"2024-07-25T03:47:07.110123Z","shell.execute_reply.started":"2024-07-25T03:47:07.103947Z","shell.execute_reply":"2024-07-25T03:47:07.108659Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:47:12.305836Z","iopub.execute_input":"2024-07-25T03:47:12.306642Z","iopub.status.idle":"2024-07-25T03:47:12.312766Z","shell.execute_reply.started":"2024-07-25T03:47:12.306603Z","shell.execute_reply":"2024-07-25T03:47:12.311302Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:47:17.006778Z","iopub.execute_input":"2024-07-25T03:47:17.007142Z","iopub.status.idle":"2024-07-25T03:49:00.643723Z","shell.execute_reply.started":"2024-07-25T03:47:17.007113Z","shell.execute_reply":"2024-07-25T03:49:00.642666Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c716cacde9d4cd991ace659ef7d0e93"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:11.587950Z","iopub.execute_input":"2024-07-25T03:49:11.588313Z","iopub.status.idle":"2024-07-25T03:49:12.109828Z","shell.execute_reply.started":"2024-07-25T03:49:11.588284Z","shell.execute_reply":"2024-07-25T03:49:12.108737Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:15.516762Z","iopub.execute_input":"2024-07-25T03:49:15.517483Z","iopub.status.idle":"2024-07-25T03:49:16.367238Z","shell.execute_reply.started":"2024-07-25T03:49:15.517451Z","shell.execute_reply":"2024-07-25T03:49:16.366268Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:19.146897Z","iopub.execute_input":"2024-07-25T03:49:19.147262Z","iopub.status.idle":"2024-07-25T03:49:24.333978Z","shell.execute_reply.started":"2024-07-25T03:49:19.147232Z","shell.execute_reply":"2024-07-25T03:49:24.332791Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257142fdb30e468480726146e0c3076a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7ae12c08264ac286b5accd2897ac84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51577ffa2d334bd5ac175528422412f1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a14800b5b3f489c9329414222a62409"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|im_end|>\\n<|im_start|>assistant\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:50.284829Z","iopub.execute_input":"2024-07-25T03:49:50.285304Z","iopub.status.idle":"2024-07-25T03:49:50.305041Z","shell.execute_reply.started":"2024-07-25T03:49:50.285269Z","shell.execute_reply":"2024-07-25T03:49:50.304074Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:54.084931Z","iopub.execute_input":"2024-07-25T03:49:54.085307Z","iopub.status.idle":"2024-07-25T03:49:54.115973Z","shell.execute_reply.started":"2024-07-25T03:49:54.085277Z","shell.execute_reply":"2024-07-25T03:49:54.115206Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:49:57.420935Z","iopub.execute_input":"2024-07-25T03:49:57.421288Z","iopub.status.idle":"2024-07-25T03:49:58.650709Z","shell.execute_reply.started":"2024-07-25T03:49:57.421261Z","shell.execute_reply":"2024-07-25T03:49:58.649670Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82dc6a834d3d4b2d8d64ce64de1dbe7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d411e020bec04e29a2a010d18b308788"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T03:50:05.434685Z","iopub.execute_input":"2024-07-25T03:50:05.435564Z","iopub.status.idle":"2024-07-25T04:19:34.138292Z","shell.execute_reply.started":"2024-07-25T03:50:05.435528Z","shell.execute_reply":"2024-07-25T04:19:34.137439Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 29:21, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>2.399600</td>\n      <td>2.558473</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.711100</td>\n      <td>2.495456</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.899900</td>\n      <td>2.466662</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.698300</td>\n      <td>2.441250</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.105800</td>\n      <td>2.425285</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=2.540288424756792, metrics={'train_runtime': 1767.7932, 'train_samples_per_second': 0.509, 'train_steps_per_second': 0.255, 'total_flos': 9288634487635968.0, 'train_loss': 2.540288424756792, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:19:56.725138Z","iopub.execute_input":"2024-07-25T04:19:56.725816Z","iopub.status.idle":"2024-07-25T04:20:00.738801Z","shell.execute_reply.started":"2024-07-25T04:19:56.725783Z","shell.execute_reply":"2024-07-25T04:20:00.738107Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>▆██▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁██</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁██</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▃▇▄▅▂▄▄▄▁▃▃▃▂▂▂▂▅▁▃▃▂▁▂▂▂█▂▂▃▃▂▃▂▂▇▂▃▂▅</td></tr><tr><td>train/learning_rate</td><td>▄███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▅▆▇▄▆▆█▅▇▅▅▆▅▇▄▄▅▇▆▅▅▅▅▇▅▂▃▅▃▁▆▆▅▅▃▆▅▆▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.42528</td></tr><tr><td>eval/runtime</td><td>74.4162</td></tr><tr><td>eval/samples_per_second</td><td>1.344</td></tr><tr><td>eval/steps_per_second</td><td>1.344</td></tr><tr><td>total_flos</td><td>9288634487635968.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>3.93272</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.1058</td></tr><tr><td>train_loss</td><td>2.54029</td></tr><tr><td>train_runtime</td><td>1767.7932</td></tr><tr><td>train_samples_per_second</td><td>0.509</td></tr><tr><td>train_steps_per_second</td><td>0.255</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gallant-night-7</strong> at: <a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5djlbs1k' target=\"_blank\">https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5djlbs1k</a><br/> View project at: <a href='https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/secaio-software/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240725_034641-5djlbs1k/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have eat to much and my belly hurts now. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:20:45.138323Z","iopub.execute_input":"2024-07-25T04:20:45.139164Z","iopub.status.idle":"2024-07-25T04:21:00.842187Z","shell.execute_reply.started":"2024-07-25T04:20:45.139134Z","shell.execute_reply":"2024-07-25T04:21:00.841299Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\nHello. I have gone through your query and understand your concern. I would like to know that how long you have been having this problem? Have you tried any treatment for this? If yes, then what was the treatment and how long did you take it? If no, then I would suggest you to start with some antacids like Omeprazole or Pantoprazole. You can take it for a week and see if it helps. If not, then you can consult a gastroenterologist and get evaluated. Hope I have answered your query. Let me know if I can assist\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have a bad acne on my face. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:22:56.756445Z","iopub.execute_input":"2024-07-25T04:22:56.756808Z","iopub.status.idle":"2024-07-25T04:23:12.681944Z","shell.execute_reply.started":"2024-07-25T04:22:56.756780Z","shell.execute_reply":"2024-07-25T04:23:12.680988Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nHello. I have gone through your query and understand your concern. Acne is a common problem and can be treated with the help of a dermatologist. I would suggest you to use a topical retinoid cream like Adapalene or Tretinoin along with a broad spectrum sunscreen like Cetaphil. You can also use a spot treatment like Benzoyl peroxide or Clindamycin. Avoid using harsh products and do not pick or squeeze the acne. You can also use a face wash containing salicylic acid. If the acne is severe, you can consult a dermatologist\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hey doc, do you speak Portuguese?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:24:38.144481Z","iopub.execute_input":"2024-07-25T04:24:38.144864Z","iopub.status.idle":"2024-07-25T04:24:55.608594Z","shell.execute_reply.started":"2024-07-25T04:24:38.144835Z","shell.execute_reply":"2024-07-25T04:24:55.607648Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\nHello, I can understand and answer your question in English. I am not a native Portuguese speaker, but I can understand and answer your question in English. I hope I have answered your query. Let me know if I can assist you further. Regards, Dr. Praveen Tayal, General & Family Physician. For more information consult a general & family physician online --> https://www.lybrate.com/anesthesiologist/dr-praveen-tayal/ --> https://www.lybrate.com/consultant/dr-praveen-tayal/ --> https://www.lybrate.com/consultant/dr-praveen-tayal/ --> https://www.ly\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-25T04:25:32.101231Z","iopub.execute_input":"2024-07-25T04:25:32.102082Z","iopub.status.idle":"2024-07-25T04:26:54.312048Z","shell.execute_reply.started":"2024-07-25T04:25:32.102050Z","shell.execute_reply":"2024-07-25T04:26:54.311101Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e85c07d07b74941b3ba51bc601853b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105a6e3f5f4f44ea91209a9c77aa8f21"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/secaio/llama-3-8b-chat-doctor/commit/a3adbdcccf72534f0d23db0bac81adee7d8c45b5', commit_message='Upload model', commit_description='', oid='a3adbdcccf72534f0d23db0bac81adee7d8c45b5', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}